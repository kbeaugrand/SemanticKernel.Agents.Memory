using System.Collections.Generic;

namespace SemanticKernel.Agents.Memory.Core;

/// <summary>
/// Configuration options for SearchClient
/// </summary>
public class SearchClientOptions
{
    /// <summary>
    /// Number between 0.0 and 2.0. It controls the randomness of the completion.
    /// The higher the temperature, the more random the completion.
    /// </summary>
    public double Temperature { get; set; } = 0;

    /// <summary>
    /// Number between 0.0 and 2.0. It controls the diversity of the completion.
    /// The higher the TopP, the more diverse the completion.
    /// </summary>
    public double TopP { get; set; } = 0;

    /// <summary>
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether
    /// they appear in the text so far, increasing the model's likelihood to talk about
    /// new topics.
    /// </summary>
    public double PresencePenalty { get; set; } = 0;

    /// <summary>
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    /// existing frequency in the text so far, decreasing the model's likelihood to repeat
    /// the same line verbatim.
    /// </summary>
    public double FrequencyPenalty { get; set; } = 0;

    /// <summary>
    /// Up to 4 sequences where the completion will stop generating further tokens.
    /// </summary>
    public IList<string> StopSequences { get; set; } = [];

    /// <summary>
    /// Maximum number of tokens accepted by the LLM used to generate answers.
    /// The number includes the tokens used for the answer, e.g. when using
    /// GPT4-32k, set this number to 32768.
    /// If the value is not set or less than one, SearchClient will use the
    /// max amount of tokens supported by the model in use.
    /// </summary>
    public int MaxAskPromptSize { get; set; } = -1;

    /// <summary>
    /// Maximum number of relevant sources to consider when generating an answer.
    /// The value is also used as the max number of results returned by SearchAsync
    /// when passing a limit less or equal to zero.
    /// </summary>
    public int MaxMatchesCount { get; set; } = 100;

    /// <summary>
    /// How many tokens to reserve for the answer generated by the LLM.
    /// E.g. if the LLM supports max 4000 tokens, and AnswerTokens is 300, then
    /// the prompt sent to LLM will contain max 3700 tokens, composed by
    /// prompt + question + grounding information retrieved from memory.
    /// </summary>
    public int AnswerTokens { get; set; } = 300;

    /// <summary>
    /// Text to return when the LLM cannot produce an answer.
    /// </summary>
    public string EmptyAnswer { get; set; } = "INFO NOT FOUND";

    /// <summary>
    /// Template use to inject facts into the RAG prompt.
    /// Available placeholders:
    /// * {{$content}}  : text from memory, i.e. chunk of text extracted from the source
    /// * {{$source}}   : name of the source file, or URL of the web page, where the content originated.
    /// * {{$relevance}}: relevance score of the current chunk of text
    /// * {{$memoryId}} : ID of the memory record
    /// * {{$tags}}     : list of tags, excluding reserved/internal ones
    /// * {{$tag[X]}}   : tag X value(s), replaced with "-" if the value is empty
    /// * {{$meta[X]}}  : value of memory record payload X field (memory payload is also known as metadata), replaced with "-" if the value is empty
    /// </summary>
    public string FactTemplate { get; set; } = "==== [File:{{$source}};Relevance:{{$relevance}}]:\n{{$content}}";
}
